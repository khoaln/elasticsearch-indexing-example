{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3, Indexing\n",
    "\n",
    "In this notebook you will index DBpedia (see the sub-collections listed under `https://github.com/uis-dat640-fall2019/admin/tree/master/assignments/assignment-3#data`). \n",
    "\n",
    "Make sure you specify the index settings, analyzer, and fields appropriately for to support the models to be implemented in subsequent notebooks.\n",
    "\n",
    "Note: you'll need to build a positional index. Use a single shard to make sure you're getting the right term statistics.\n",
    "\n",
    "Be sure to use both markdown cells with section headings and explanations, as well as writing readable code, to make it clear what your intention is each step of the way through the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "import os\n",
    "import bz2\n",
    "from rdflib.plugins.parsers.ntriples import NTriplesParser\n",
    "from rdflib.plugins.parsers.ntriples import ParseError\n",
    "from rdflib.term import URIRef\n",
    "import re\n",
    "import operator\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.WARNING);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data file list to index and prefix mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a list of data filenames and direction of the triple (reverse or not). Due to resource intensive, only 3 files are indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "#     {'name': \"anchor_text_en.ttl.bz2\"},\n",
    "#     {'name': \"article_categories_en.ttl.bz2\"},\n",
    "#     {'name': \"disambiguations_en.ttl.bz2\", 'reverse': True}, \n",
    "#     {'name': \"infobox_properties_en.ttl.bz2\"},\n",
    "#     {'name': \"instance_types_transitive_en.ttl.bz2\"},\n",
    "    {'name': \"labels_en.ttl.bz2\"},\n",
    "    {'name': \"long_abstracts_en.ttl.bz2\"},\n",
    "#     {'name': \"mappingbased_literals_en.ttl.bz2\"},\n",
    "#     {'name': \"mappingbased_objects_en.ttl.bz2\"},\n",
    "    {'name': \"page_links_en.ttl.bz2\"},\n",
    "#     {'name': \"persondata_en.ttl.bz2\"},\n",
    "#     {'name': \"short_abstracts_en.ttl.bz2\"},\n",
    "#     {'name': \"transitive_redirects_en.ttl.bz2\", 'reverse': True}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a namespace prefixes, referenced from the book https://link.springer.com/content/pdf/10.1007%2F978-3-319-93935-3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = {\n",
    "    'http://dbpedia.org/resource/': \"dbpedia\",\n",
    "    'http://dbpedia.org/property/': \"dbp\",\n",
    "    'http://dbpedia.org/ontology/': \"dbo\",\n",
    "    'http://purl.org/dc/elements/1.1/': \"dc\",\n",
    "    'http://xmlns.com/foaf/0.1/': \"foaf\",\n",
    "    'http://www.georss.org/georss/': \"georss\",\n",
    "    'http://www.w3.org/2002/07/owl#': \"owl\",\n",
    "    'http://www.w3.org/1999/02/22-rdf-syntax-ns#': \"rdf\",\n",
    "    'http://www.w3.org/2000/01/rdf-schema#': \"rdfs\",\n",
    "    'http://purl.org/dc/terms/': \"dcterms\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the Knowledge base files and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class to hold triple values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPO():\n",
    "    def __init__(self):\n",
    "        self.s = None\n",
    "        self.p = None\n",
    "        self.o = None\n",
    "        \n",
    "    def triple(self, s, p, o):\n",
    "        self.s = s\n",
    "        self.p = p\n",
    "        self.o = o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Util functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert an url into prefixed entity\n",
    "def url_to_prefixed(url):\n",
    "    # find the last # or /\n",
    "    k = url.rfind('#')\n",
    "    key = ''\n",
    "    if k >= 0:\n",
    "        key = url[:k+1]\n",
    "    else:\n",
    "        k = url.rfind('/')\n",
    "        if k >= 0:\n",
    "            key = url[:k+1]\n",
    "    prefixed = url\n",
    "    if key in prefixes:\n",
    "        prefixed = url.replace(key, prefixes[key] + \":\")\n",
    "    \n",
    "    if prefixed[0] != '<':\n",
    "        prefixed = '<' + prefixed\n",
    "    if prefixed[-1] != '>':\n",
    "        prefixed += '>'\n",
    "    \n",
    "    return prefixed\n",
    "\n",
    "# Function to parse a file and return a set of subjects\n",
    "def get_set_subjects(file):\n",
    "    filepath = os.path.join(DATA_PATH, DBPEDIA_PATH, file['name'])\n",
    "    reverse = file.get('reverse', False)\n",
    "    spo = SPO()\n",
    "    parser = NTriplesParser(spo)\n",
    "    count = 0\n",
    "    i = 0\n",
    "    subjects = set()\n",
    "    with bz2.open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            count += 1\n",
    "            if count == 1000000:\n",
    "                i += 1\n",
    "                count = 0\n",
    "                print(\"File {}: {} million lines processed\".format(file['name'], i))\n",
    "            try:\n",
    "                # parse the triple\n",
    "                parser.parsestring(line.decode(\"utf-8\"))\n",
    "            except ParseError:\n",
    "                continue\n",
    "                \n",
    "            if spo.s is None or spo.o is None:\n",
    "                continue\n",
    "                \n",
    "            subject = url_to_prefixed(spo.s)\n",
    "            object = spo.o\n",
    "            # if object is a URI\n",
    "            if type(object) is URIRef:\n",
    "                object = url_to_prefixed(object)\n",
    "            \n",
    "            # reverse direction\n",
    "            if reverse:\n",
    "                temp = subject\n",
    "                subject = object\n",
    "                object = temp\n",
    "                \n",
    "            # only keep subject that is dbpedia entity\n",
    "            if not subject.startswith('<dbpedia:'):\n",
    "                continue\n",
    "                \n",
    "            subjects.add(subject)\n",
    "    \n",
    "    return subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define data path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "DBPEDIA_PATH = \"dbpedia\"\n",
    "\n",
    "ENTITIES = set()\n",
    "PREDICATES_COUNT = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the file labels_en.ttl.bz2 and short_abstracts_en.ttl.bz2 to get a set of entity that have both <rdfs:label> and <rdfs:comment>. We then will ignore entities from other files that not in this set. Using pickle to dump the set of entities for later use without parsing the files again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File labels_en.ttl.bz2: 1 million lines processed\n",
      "File labels_en.ttl.bz2: 2 million lines processed\n",
      "File labels_en.ttl.bz2: 3 million lines processed\n",
      "File labels_en.ttl.bz2: 4 million lines processed\n",
      "File labels_en.ttl.bz2: 5 million lines processed\n",
      "File labels_en.ttl.bz2: 6 million lines processed\n",
      "File labels_en.ttl.bz2: 7 million lines processed\n",
      "File labels_en.ttl.bz2: 8 million lines processed\n",
      "File labels_en.ttl.bz2: 9 million lines processed\n",
      "File labels_en.ttl.bz2: 10 million lines processed\n",
      "File labels_en.ttl.bz2: 11 million lines processed\n",
      "File short_abstracts_en.ttl.bz2: 1 million lines processed\n",
      "File short_abstracts_en.ttl.bz2: 2 million lines processed\n",
      "File short_abstracts_en.ttl.bz2: 3 million lines processed\n",
      "File short_abstracts_en.ttl.bz2: 4 million lines processed\n"
     ]
    }
   ],
   "source": [
    "ENTITIES = get_set_subjects({'name': 'labels_en.ttl.bz2'})\n",
    "ENTITIES = ENTITIES.intersection(get_set_subjects({'name': 'short_abstracts_en.ttl.bz2'}))\n",
    "\n",
    "f = open(b'data/entities.pkl', 'wb')\n",
    "pickle.dump(ENTITIES, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4630608"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload entities if needed\n",
    "ENTITIES = pickle.load(open('data/entities.bk.pkl', 'rb'))\n",
    "len(ENTITIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define index name and settings for term-based index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"dbpedia_index2\"  \n",
    "\n",
    "INDEX_SETTINGS = {\n",
    "    'settings' : {\n",
    "        'index' : {\n",
    "            \"number_of_shards\" : 1,\n",
    "            \"number_of_replicas\" : 0,\n",
    "            \"blocks\": {\n",
    "                \"read_only_allow_delete\": \"false\"\n",
    "            }\n",
    "        },\n",
    "        'analysis': {\n",
    "            'analyzer': {\n",
    "                'my_english_analyzer': {\n",
    "                    'type': \"custom\",\n",
    "                    'tokenizer': \"standard\",\n",
    "                    'stopwords': \"_english_\",\n",
    "                    'filter': [\n",
    "                        \"lowercase\",\n",
    "                        \"english_stop\",\n",
    "                        \"filter_english_minimal\"\n",
    "                    ]                \n",
    "                }\n",
    "            },\n",
    "            'filter' : {\n",
    "                'filter_english_minimal' : {\n",
    "                    'type': \"stemmer\",\n",
    "                    'name': \"minimal_english\"\n",
    "                },\n",
    "                'english_stop': {\n",
    "                    'type': \"stop\",\n",
    "                    'stopwords': \"_english_\"\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'catch-all': {\n",
    "                'type': \"text\",\n",
    "                'term_vector': \"with_positions\",\n",
    "                'analyzer': \"my_english_analyzer\"\n",
    "            },\n",
    "            'abstract': {\n",
    "                'type': \"text\",\n",
    "                'term_vector': \"with_positions\",\n",
    "                'analyzer': \"my_english_analyzer\"\n",
    "            },\n",
    "            'names': {\n",
    "                'type': \"text\",\n",
    "                'term_vector': \"with_positions\",\n",
    "                'analyzer': \"my_english_analyzer\"\n",
    "            },\n",
    "            'categories': {\n",
    "                'type': \"text\",\n",
    "                'term_vector': \"with_positions\",\n",
    "                'analyzer': \"my_english_analyzer\"\n",
    "            },\n",
    "            'attributes': {\n",
    "                'type': \"text\",\n",
    "                'term_vector': \"with_positions\",\n",
    "                'analyzer': \"my_english_analyzer\"\n",
    "            },\n",
    "            'similar_entity_names': {\n",
    "                'type': \"text\",\n",
    "                'term_vector': \"with_positions\",\n",
    "                'analyzer': \"my_english_analyzer\"\n",
    "            },\n",
    "            'related_entity_names': {\n",
    "                'type': \"text\",\n",
    "                'term_vector': \"with_positions\",\n",
    "                'analyzer': \"my_english_analyzer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create term-based index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(timeout=30, max_retries=10, retry_on_timeout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'dbpedia_index2'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if es.indices.exists(INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "    \n",
    "es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicates constraints for the fields\n",
    "names_predicates = [\"<foaf:name>\", \"<dbp:name>\", \"<foaf:givenName>\", \"<foaf:surname>\", \"<dbp:officialName>\", \n",
    "                    \"<dbp:fullname>\", \"<dbp:nativeName>\", \"<dbp:birthName>\", \"<dbo:birthName>\", \"<dbp:nickname>\",\n",
    "                    \"<dbp:showName>\", \"<dbp:shipName>\", \"<dbp:clubname>\", \"<dbp:unitName>\", \"<dbp:otherName>\",\n",
    "                    \"<dbo:formerName>\", \"<dbp:birthname>\", \"<dbp:alternativeNames>\", \"<dbp:otherNames>\", \"<dbp:names>\",\n",
    "                    \"<rdfs:label>\"]\n",
    "categories_predicates = [\"<dcterms:subject>\"]\n",
    "similar_entity_names_predicates = [\"!<dbo:wikiPageRedirects>\", \"!<dbo:wikiPageDisambiguates>\", \n",
    "                                   \"<dbo:wikiPageWikiLinkText>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that used to resolve the URIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to process uri to text: removing prefix, convert camelCase into \"camel case\"\n",
    "def URI_to_text(uri):\n",
    "    text = str(uri)\n",
    "    \n",
    "    # remove brackets\n",
    "    text = text.replace('<', '').replace('>', '')\n",
    "    if text.startswith('dbpedia:Category') or text.startswith('dbpedia:File'): # resolve Category URIs\n",
    "        k = text.rfind(':')\n",
    "        text = text[k + 1:]\n",
    "    elif text.startswith('http'):\n",
    "        k = text.rfind('/')\n",
    "        text = text[k + 1:]\n",
    "    else:\n",
    "        k = text.find(':')\n",
    "        text = text[k + 1:]\n",
    "    \n",
    "    # break camelCase\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1 \\2', text)\n",
    "    text = re.sub('([a-z0-9])([A-Z])', r'\\1 \\2', s1)\n",
    "    \n",
    "    return text.strip()\n",
    "    \n",
    "\n",
    "# Function to do the URI resolution, given a predicate and a list of objects\n",
    "def URI_resolution(predicate, object_list):\n",
    "    resolved = []\n",
    "    for obj in object_list:\n",
    "        obj = str(obj)\n",
    "        if obj.startswith('<') and obj.endswith('>'): # URI object\n",
    "            obj = URI_to_text(obj)\n",
    "        # if p matches <dbp:.*> both p and o are stored (i.e. \"p o\" is indexed).\n",
    "        if predicate.startswith('<dbp:'):\n",
    "            obj = \"{} {}\".format(URI_to_text(predicate), obj)\n",
    "        \n",
    "        # replace specific characters with space\n",
    "        chars = \"'.,!?()\\\"$#;`~@%^&*_+=[]{}|\\\\-\"\n",
    "        for ch in chars:\n",
    "            if ch in obj:\n",
    "                obj = obj.replace(ch, ' ')\n",
    "        obj = re.sub(' +', ' ', obj)\n",
    "        resolved.append(obj)\n",
    "    return resolved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the files that defined above and building the index. Using actions bulk from Elastisearch helper to improve the performance. Also counting the predicate occurrence to get the top-predicate for entity-based indexing later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File labels_en.ttl.bz2: 1 million lines processed\n",
      "File labels_en.ttl.bz2: 2 million lines processed\n",
      "File labels_en.ttl.bz2: 3 million lines processed\n",
      "File labels_en.ttl.bz2: 4 million lines processed\n",
      "File labels_en.ttl.bz2: 5 million lines processed\n",
      "File labels_en.ttl.bz2: 6 million lines processed\n",
      "File labels_en.ttl.bz2: 7 million lines processed\n",
      "File labels_en.ttl.bz2: 8 million lines processed\n",
      "File labels_en.ttl.bz2: 9 million lines processed\n",
      "File labels_en.ttl.bz2: 10 million lines processed\n",
      "File labels_en.ttl.bz2: 11 million lines processed\n",
      "File long_abstracts_en.ttl.bz2: 1 million lines processed\n",
      "File long_abstracts_en.ttl.bz2: 2 million lines processed\n",
      "File long_abstracts_en.ttl.bz2: 3 million lines processed\n",
      "File long_abstracts_en.ttl.bz2: 4 million lines processed\n",
      "File page_links_en.ttl.bz2: 1 million lines processed\n",
      "File page_links_en.ttl.bz2: 2 million lines processed\n",
      "File page_links_en.ttl.bz2: 3 million lines processed\n",
      "File page_links_en.ttl.bz2: 4 million lines processed\n",
      "File page_links_en.ttl.bz2: 5 million lines processed\n",
      "File page_links_en.ttl.bz2: 6 million lines processed\n",
      "File page_links_en.ttl.bz2: 7 million lines processed\n",
      "File page_links_en.ttl.bz2: 8 million lines processed\n",
      "File page_links_en.ttl.bz2: 9 million lines processed\n",
      "File page_links_en.ttl.bz2: 10 million lines processed\n",
      "File page_links_en.ttl.bz2: 11 million lines processed\n",
      "File page_links_en.ttl.bz2: 12 million lines processed\n",
      "File page_links_en.ttl.bz2: 13 million lines processed\n",
      "File page_links_en.ttl.bz2: 14 million lines processed\n",
      "File page_links_en.ttl.bz2: 15 million lines processed\n",
      "File page_links_en.ttl.bz2: 16 million lines processed\n",
      "File page_links_en.ttl.bz2: 17 million lines processed\n",
      "File page_links_en.ttl.bz2: 18 million lines processed\n",
      "File page_links_en.ttl.bz2: 19 million lines processed\n",
      "File page_links_en.ttl.bz2: 20 million lines processed\n",
      "File page_links_en.ttl.bz2: 21 million lines processed\n",
      "File page_links_en.ttl.bz2: 22 million lines processed\n",
      "File page_links_en.ttl.bz2: 23 million lines processed\n",
      "File page_links_en.ttl.bz2: 24 million lines processed\n",
      "File page_links_en.ttl.bz2: 25 million lines processed\n",
      "File page_links_en.ttl.bz2: 26 million lines processed\n",
      "File page_links_en.ttl.bz2: 27 million lines processed\n",
      "File page_links_en.ttl.bz2: 28 million lines processed\n",
      "File page_links_en.ttl.bz2: 29 million lines processed\n",
      "File page_links_en.ttl.bz2: 30 million lines processed\n",
      "File page_links_en.ttl.bz2: 31 million lines processed\n",
      "File page_links_en.ttl.bz2: 32 million lines processed\n",
      "File page_links_en.ttl.bz2: 33 million lines processed\n",
      "File page_links_en.ttl.bz2: 34 million lines processed\n",
      "File page_links_en.ttl.bz2: 35 million lines processed\n",
      "File page_links_en.ttl.bz2: 36 million lines processed\n",
      "File page_links_en.ttl.bz2: 37 million lines processed\n",
      "File page_links_en.ttl.bz2: 38 million lines processed\n",
      "File page_links_en.ttl.bz2: 39 million lines processed\n",
      "File page_links_en.ttl.bz2: 40 million lines processed\n",
      "File page_links_en.ttl.bz2: 41 million lines processed\n",
      "File page_links_en.ttl.bz2: 42 million lines processed\n",
      "File page_links_en.ttl.bz2: 43 million lines processed\n",
      "File page_links_en.ttl.bz2: 44 million lines processed\n",
      "File page_links_en.ttl.bz2: 45 million lines processed\n",
      "File page_links_en.ttl.bz2: 46 million lines processed\n",
      "File page_links_en.ttl.bz2: 47 million lines processed\n",
      "File page_links_en.ttl.bz2: 48 million lines processed\n",
      "File page_links_en.ttl.bz2: 49 million lines processed\n",
      "File page_links_en.ttl.bz2: 50 million lines processed\n",
      "File page_links_en.ttl.bz2: 51 million lines processed\n",
      "File page_links_en.ttl.bz2: 52 million lines processed\n",
      "File page_links_en.ttl.bz2: 53 million lines processed\n",
      "File page_links_en.ttl.bz2: 54 million lines processed\n",
      "File page_links_en.ttl.bz2: 55 million lines processed\n",
      "File page_links_en.ttl.bz2: 56 million lines processed\n",
      "File page_links_en.ttl.bz2: 57 million lines processed\n",
      "File page_links_en.ttl.bz2: 58 million lines processed\n",
      "File page_links_en.ttl.bz2: 59 million lines processed\n",
      "File page_links_en.ttl.bz2: 60 million lines processed\n",
      "File page_links_en.ttl.bz2: 61 million lines processed\n",
      "File page_links_en.ttl.bz2: 62 million lines processed\n",
      "File page_links_en.ttl.bz2: 63 million lines processed\n",
      "File page_links_en.ttl.bz2: 64 million lines processed\n",
      "File page_links_en.ttl.bz2: 65 million lines processed\n",
      "File page_links_en.ttl.bz2: 66 million lines processed\n",
      "File page_links_en.ttl.bz2: 67 million lines processed\n",
      "File page_links_en.ttl.bz2: 68 million lines processed\n",
      "File page_links_en.ttl.bz2: 69 million lines processed\n",
      "File page_links_en.ttl.bz2: 70 million lines processed\n",
      "File page_links_en.ttl.bz2: 71 million lines processed\n",
      "File page_links_en.ttl.bz2: 72 million lines processed\n",
      "File page_links_en.ttl.bz2: 73 million lines processed\n",
      "File page_links_en.ttl.bz2: 74 million lines processed\n",
      "File page_links_en.ttl.bz2: 75 million lines processed\n",
      "File page_links_en.ttl.bz2: 76 million lines processed\n",
      "File page_links_en.ttl.bz2: 77 million lines processed\n",
      "File page_links_en.ttl.bz2: 78 million lines processed\n",
      "File page_links_en.ttl.bz2: 79 million lines processed\n",
      "File page_links_en.ttl.bz2: 80 million lines processed\n",
      "File page_links_en.ttl.bz2: 81 million lines processed\n",
      "File page_links_en.ttl.bz2: 82 million lines processed\n",
      "File page_links_en.ttl.bz2: 83 million lines processed\n",
      "File page_links_en.ttl.bz2: 84 million lines processed\n",
      "File page_links_en.ttl.bz2: 85 million lines processed\n",
      "File page_links_en.ttl.bz2: 86 million lines processed\n",
      "File page_links_en.ttl.bz2: 87 million lines processed\n",
      "File page_links_en.ttl.bz2: 88 million lines processed\n",
      "File page_links_en.ttl.bz2: 89 million lines processed\n",
      "File page_links_en.ttl.bz2: 90 million lines processed\n",
      "File page_links_en.ttl.bz2: 91 million lines processed\n",
      "File page_links_en.ttl.bz2: 92 million lines processed\n",
      "File page_links_en.ttl.bz2: 93 million lines processed\n",
      "File page_links_en.ttl.bz2: 94 million lines processed\n",
      "File page_links_en.ttl.bz2: 95 million lines processed\n",
      "File page_links_en.ttl.bz2: 96 million lines processed\n",
      "File page_links_en.ttl.bz2: 97 million lines processed\n",
      "File page_links_en.ttl.bz2: 98 million lines processed\n",
      "File page_links_en.ttl.bz2: 99 million lines processed\n",
      "File page_links_en.ttl.bz2: 100 million lines processed\n",
      "File page_links_en.ttl.bz2: 101 million lines processed\n",
      "File page_links_en.ttl.bz2: 102 million lines processed\n",
      "File page_links_en.ttl.bz2: 103 million lines processed\n",
      "File page_links_en.ttl.bz2: 104 million lines processed\n",
      "File page_links_en.ttl.bz2: 105 million lines processed\n",
      "File page_links_en.ttl.bz2: 106 million lines processed\n",
      "File page_links_en.ttl.bz2: 107 million lines processed\n",
      "File page_links_en.ttl.bz2: 108 million lines processed\n",
      "File page_links_en.ttl.bz2: 109 million lines processed\n",
      "File page_links_en.ttl.bz2: 110 million lines processed\n",
      "File page_links_en.ttl.bz2: 111 million lines processed\n",
      "File page_links_en.ttl.bz2: 112 million lines processed\n",
      "File page_links_en.ttl.bz2: 113 million lines processed\n",
      "File page_links_en.ttl.bz2: 114 million lines processed\n",
      "File page_links_en.ttl.bz2: 115 million lines processed\n",
      "File page_links_en.ttl.bz2: 116 million lines processed\n",
      "File page_links_en.ttl.bz2: 117 million lines processed\n",
      "File page_links_en.ttl.bz2: 118 million lines processed\n",
      "File page_links_en.ttl.bz2: 119 million lines processed\n",
      "File page_links_en.ttl.bz2: 120 million lines processed\n",
      "File page_links_en.ttl.bz2: 121 million lines processed\n",
      "File page_links_en.ttl.bz2: 122 million lines processed\n",
      "File page_links_en.ttl.bz2: 123 million lines processed\n",
      "File page_links_en.ttl.bz2: 124 million lines processed\n",
      "File page_links_en.ttl.bz2: 125 million lines processed\n",
      "File page_links_en.ttl.bz2: 126 million lines processed\n",
      "File page_links_en.ttl.bz2: 127 million lines processed\n",
      "File page_links_en.ttl.bz2: 128 million lines processed\n",
      "File page_links_en.ttl.bz2: 129 million lines processed\n",
      "File page_links_en.ttl.bz2: 130 million lines processed\n",
      "File page_links_en.ttl.bz2: 131 million lines processed\n",
      "File page_links_en.ttl.bz2: 132 million lines processed\n",
      "File page_links_en.ttl.bz2: 133 million lines processed\n",
      "File page_links_en.ttl.bz2: 134 million lines processed\n",
      "File page_links_en.ttl.bz2: 135 million lines processed\n",
      "File page_links_en.ttl.bz2: 136 million lines processed\n",
      "File page_links_en.ttl.bz2: 137 million lines processed\n",
      "File page_links_en.ttl.bz2: 138 million lines processed\n",
      "File page_links_en.ttl.bz2: 139 million lines processed\n",
      "File page_links_en.ttl.bz2: 140 million lines processed\n",
      "File page_links_en.ttl.bz2: 141 million lines processed\n",
      "File page_links_en.ttl.bz2: 142 million lines processed\n",
      "File page_links_en.ttl.bz2: 143 million lines processed\n",
      "File page_links_en.ttl.bz2: 144 million lines processed\n",
      "File page_links_en.ttl.bz2: 145 million lines processed\n",
      "File page_links_en.ttl.bz2: 146 million lines processed\n",
      "File page_links_en.ttl.bz2: 147 million lines processed\n",
      "File page_links_en.ttl.bz2: 148 million lines processed\n",
      "File page_links_en.ttl.bz2: 149 million lines processed\n",
      "File page_links_en.ttl.bz2: 150 million lines processed\n",
      "File page_links_en.ttl.bz2: 151 million lines processed\n",
      "File page_links_en.ttl.bz2: 152 million lines processed\n",
      "File page_links_en.ttl.bz2: 153 million lines processed\n",
      "File page_links_en.ttl.bz2: 154 million lines processed\n",
      "File page_links_en.ttl.bz2: 155 million lines processed\n",
      "File page_links_en.ttl.bz2: 156 million lines processed\n",
      "File page_links_en.ttl.bz2: 157 million lines processed\n",
      "File page_links_en.ttl.bz2: 158 million lines processed\n",
      "File page_links_en.ttl.bz2: 159 million lines processed\n",
      "File page_links_en.ttl.bz2: 160 million lines processed\n",
      "File page_links_en.ttl.bz2: 161 million lines processed\n",
      "File page_links_en.ttl.bz2: 162 million lines processed\n",
      "File page_links_en.ttl.bz2: 163 million lines processed\n",
      "File page_links_en.ttl.bz2: 164 million lines processed\n",
      "File page_links_en.ttl.bz2: 165 million lines processed\n",
      "File page_links_en.ttl.bz2: 166 million lines processed\n",
      "File page_links_en.ttl.bz2: 167 million lines processed\n",
      "File page_links_en.ttl.bz2: 168 million lines processed\n",
      "File page_links_en.ttl.bz2: 169 million lines processed\n",
      "File page_links_en.ttl.bz2: 170 million lines processed\n",
      "Indexing completed\n"
     ]
    }
   ],
   "source": [
    "actions = []\n",
    "# hold a set of indexed entities. For the entities already in the index, doing the update action using _op_type=update\n",
    "indexed_entities = set()\n",
    "catchalls = {}\n",
    "for file in files:\n",
    "    filepath = os.path.join(DATA_PATH, DBPEDIA_PATH, file['name'])\n",
    "    reverse = file.get('reverse', False)\n",
    "    spo = SPO()\n",
    "    parser = NTriplesParser(spo)\n",
    "    count, j = 0, 0\n",
    "    \n",
    "    with bz2.open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            count += 1\n",
    "            if count == 1000000:\n",
    "                j += 1\n",
    "                count = 0\n",
    "                print(\"File {}: {} million lines processed\".format(file['name'], j))\n",
    "            try:\n",
    "                # parse the triple\n",
    "                parser.parsestring(line.decode(\"utf-8\"))\n",
    "            except ParseError:\n",
    "                continue\n",
    "            \n",
    "            if spo.s is None or spo.o is None:\n",
    "                continue\n",
    "\n",
    "            subject = url_to_prefixed(spo.s)\n",
    "                \n",
    "            predicate = url_to_prefixed(spo.p)\n",
    "            if reverse:\n",
    "                predicate = \"!\" + predicate\n",
    "            \n",
    "            object = spo.o\n",
    "            # if object is a URI\n",
    "            if type(object) is URIRef:\n",
    "                object = url_to_prefixed(object)\n",
    "            \n",
    "            # reverse direction\n",
    "            if reverse:\n",
    "                temp = subject\n",
    "                subject = object\n",
    "                object = temp\n",
    "                \n",
    "            # only keep subject that is dbpedia entity\n",
    "            if not subject.startswith('<dbpedia:'):\n",
    "                continue\n",
    "            \n",
    "            # only keep entities that has both <rdfs:label> and <rdfs:comment>\n",
    "            if subject not in ENTITIES:\n",
    "                continue\n",
    "                    \n",
    "            # Count predicates then later we can find the top predicates to consider as fields\n",
    "            if predicate not in PREDICATES_COUNT:\n",
    "                PREDICATES_COUNT[predicate] = 1\n",
    "            else:\n",
    "                PREDICATES_COUNT[predicate] += 1\n",
    "                \n",
    "            source = {}\n",
    "\n",
    "            uri_resolved = URI_resolution(predicate, [object])\n",
    "            if predicate in names_predicates: # Field Names\n",
    "                source['names'] = uri_resolved\n",
    "\n",
    "            elif predicate in categories_predicates: # Field Categories\n",
    "                source['categories'] = uri_resolved\n",
    "\n",
    "            elif predicate in similar_entity_names_predicates: # Field Similar entity names\n",
    "                source['similar_entity_names'] = uri_resolved\n",
    "\n",
    "            else:\n",
    "                if object.startswith('<') and object.endswith('>'): # object is a URI => Field Related entity names\n",
    "                    source['related_entity_names'] = uri_resolved\n",
    "                else:\n",
    "                    # Field Attributes\n",
    "                    source['attributes'] = uri_resolved\n",
    "                    \n",
    "            # Field catch-all\n",
    "            if subject in catchalls:\n",
    "                catchalls[subject] += uri_resolved\n",
    "            else:\n",
    "                catchalls[subject] = uri_resolved\n",
    "                \n",
    "            # Field abstract\n",
    "            if predicate == '<dbo:abstract>':\n",
    "                source['abstract'] = object\n",
    "            \n",
    "            if subject in indexed_entities: # update the entity index\n",
    "                actions.append({\n",
    "                    '_index': INDEX_NAME,\n",
    "                    '_id': subject,\n",
    "                    'doc': source,\n",
    "                    '_op_type': 'update',\n",
    "                })\n",
    "            else:                           # add entity into index\n",
    "                actions.append({\n",
    "                    '_index': INDEX_NAME,\n",
    "                    '_id': subject,\n",
    "                    '_source': source\n",
    "                })\n",
    "                indexed_entities.add(subject)\n",
    "\n",
    "            # Add entities into the index in a bulk of 5000 to improve the performance\n",
    "            if len(actions) > 5000:\n",
    "                helpers.bulk(es, actions)\n",
    "                actions = []\n",
    "                \n",
    "# process the last bulk\n",
    "helpers.bulk(es, actions)\n",
    "print('Indexing completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index catch-all field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing completed\n"
     ]
    }
   ],
   "source": [
    "actions = []\n",
    "for subject in catchalls:\n",
    "    actions.append({\n",
    "        '_index': INDEX_NAME,\n",
    "        '_id': subject,\n",
    "        'doc': {'catch-all': catchalls[subject]},\n",
    "        '_op_type': 'update',\n",
    "    })\n",
    "    \n",
    "    # Add entities into the index in a bulk of 5000 to improve the performance\n",
    "    if len(actions) > 5000:\n",
    "        helpers.bulk(es, actions)\n",
    "        actions = []\n",
    "        \n",
    "# process the last bulk\n",
    "helpers.bulk(es, actions)\n",
    "print('Indexing completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the top-1000 most frequent predicates to make fields for entity-based index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_PREDICATES = sorted(PREDICATES_COUNT.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<dbo:wikiPageWikiLink>', 158426950),\n",
       " ('<dbo:abstract>', 4630609),\n",
       " ('<rdfs:label>', 4630608)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP_PREDICATES = TOP_PREDICATES[:1000]\n",
    "TOP_PREDICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(b'data/top_predicates.pkl', 'wb')\n",
    "pickle.dump(TOP_PREDICATES, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_PREDICATES = pickle.load(open('data/top_predicates.bk.pkl', 'rb'))\n",
    "TOP_PREDICATES = [field[0] for field in TOP_PREDICATES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update index settings for top-predicates fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_URI_SETTINGS = {\n",
    "    'settings' : {\n",
    "        'index' : {\n",
    "            \"number_of_shards\" : 1,\n",
    "            \"number_of_replicas\" : 0\n",
    "        }\n",
    "    },\n",
    "    'mappings': {\n",
    "        'properties': {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for predicate in TOP_PREDICATES:\n",
    "    INDEX_URI_SETTINGS['mappings']['properties'][predicate] = {\n",
    "        'type': \"text\",\n",
    "        'term_vector': \"yes\",\n",
    "        'analyzer': \"keyword\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'settings': {'index': {'number_of_shards': 1, 'number_of_replicas': 0}},\n",
       " 'mappings': {'properties': {'<dbo:wikiPageWikiLink>': {'type': 'text',\n",
       "    'term_vector': 'yes',\n",
       "    'analyzer': 'keyword'},\n",
       "   '<dbo:abstract>': {'type': 'text',\n",
       "    'term_vector': 'yes',\n",
       "    'analyzer': 'keyword'},\n",
       "   '<rdfs:label>': {'type': 'text',\n",
       "    'term_vector': 'yes',\n",
       "    'analyzer': 'keyword'}}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_URI_SETTINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the entity-based index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'dbpedia_uri_index'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_URI_NAME = 'dbpedia_uri_index'\n",
    "if es.indices.exists(INDEX_URI_NAME):\n",
    "    es.indices.delete(index=INDEX_URI_NAME)\n",
    "    \n",
    "es.indices.create(index=INDEX_URI_NAME, body=INDEX_URI_SETTINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the files that defined above and building the entity-based index with the top predicates as fields. Using actions bulk from Elastisearch helper to improve the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File labels_en.ttl.bz2: 1 million lines processed\n",
      "File labels_en.ttl.bz2: 2 million lines processed\n",
      "File labels_en.ttl.bz2: 3 million lines processed\n",
      "File labels_en.ttl.bz2: 4 million lines processed\n",
      "File labels_en.ttl.bz2: 5 million lines processed\n",
      "File labels_en.ttl.bz2: 6 million lines processed\n",
      "File labels_en.ttl.bz2: 7 million lines processed\n",
      "File labels_en.ttl.bz2: 8 million lines processed\n",
      "File labels_en.ttl.bz2: 9 million lines processed\n",
      "File labels_en.ttl.bz2: 10 million lines processed\n",
      "File labels_en.ttl.bz2: 11 million lines processed\n",
      "File long_abstracts_en.ttl.bz2: 1 million lines processed\n",
      "File long_abstracts_en.ttl.bz2: 2 million lines processed\n",
      "File long_abstracts_en.ttl.bz2: 3 million lines processed\n",
      "File long_abstracts_en.ttl.bz2: 4 million lines processed\n",
      "File page_links_en.ttl.bz2: 1 million lines processed\n",
      "File page_links_en.ttl.bz2: 2 million lines processed\n",
      "File page_links_en.ttl.bz2: 3 million lines processed\n",
      "File page_links_en.ttl.bz2: 4 million lines processed\n",
      "File page_links_en.ttl.bz2: 5 million lines processed\n",
      "File page_links_en.ttl.bz2: 6 million lines processed\n",
      "File page_links_en.ttl.bz2: 7 million lines processed\n",
      "File page_links_en.ttl.bz2: 8 million lines processed\n",
      "File page_links_en.ttl.bz2: 9 million lines processed\n",
      "File page_links_en.ttl.bz2: 10 million lines processed\n",
      "File page_links_en.ttl.bz2: 11 million lines processed\n",
      "File page_links_en.ttl.bz2: 12 million lines processed\n",
      "File page_links_en.ttl.bz2: 13 million lines processed\n",
      "File page_links_en.ttl.bz2: 14 million lines processed\n",
      "File page_links_en.ttl.bz2: 15 million lines processed\n",
      "File page_links_en.ttl.bz2: 16 million lines processed\n",
      "File page_links_en.ttl.bz2: 17 million lines processed\n",
      "File page_links_en.ttl.bz2: 18 million lines processed\n",
      "File page_links_en.ttl.bz2: 19 million lines processed\n",
      "File page_links_en.ttl.bz2: 20 million lines processed\n",
      "File page_links_en.ttl.bz2: 21 million lines processed\n",
      "File page_links_en.ttl.bz2: 22 million lines processed\n",
      "File page_links_en.ttl.bz2: 23 million lines processed\n",
      "File page_links_en.ttl.bz2: 24 million lines processed\n",
      "File page_links_en.ttl.bz2: 25 million lines processed\n",
      "File page_links_en.ttl.bz2: 26 million lines processed\n",
      "File page_links_en.ttl.bz2: 27 million lines processed\n",
      "File page_links_en.ttl.bz2: 28 million lines processed\n",
      "File page_links_en.ttl.bz2: 29 million lines processed\n",
      "File page_links_en.ttl.bz2: 30 million lines processed\n",
      "File page_links_en.ttl.bz2: 31 million lines processed\n",
      "File page_links_en.ttl.bz2: 32 million lines processed\n",
      "File page_links_en.ttl.bz2: 33 million lines processed\n",
      "File page_links_en.ttl.bz2: 34 million lines processed\n",
      "File page_links_en.ttl.bz2: 35 million lines processed\n",
      "File page_links_en.ttl.bz2: 36 million lines processed\n",
      "File page_links_en.ttl.bz2: 37 million lines processed\n",
      "File page_links_en.ttl.bz2: 38 million lines processed\n",
      "File page_links_en.ttl.bz2: 39 million lines processed\n",
      "File page_links_en.ttl.bz2: 40 million lines processed\n",
      "File page_links_en.ttl.bz2: 41 million lines processed\n",
      "File page_links_en.ttl.bz2: 42 million lines processed\n",
      "File page_links_en.ttl.bz2: 43 million lines processed\n",
      "File page_links_en.ttl.bz2: 44 million lines processed\n",
      "File page_links_en.ttl.bz2: 45 million lines processed\n",
      "File page_links_en.ttl.bz2: 46 million lines processed\n",
      "File page_links_en.ttl.bz2: 47 million lines processed\n",
      "File page_links_en.ttl.bz2: 48 million lines processed\n",
      "File page_links_en.ttl.bz2: 49 million lines processed\n",
      "File page_links_en.ttl.bz2: 50 million lines processed\n",
      "File page_links_en.ttl.bz2: 51 million lines processed\n",
      "File page_links_en.ttl.bz2: 52 million lines processed\n",
      "File page_links_en.ttl.bz2: 53 million lines processed\n",
      "File page_links_en.ttl.bz2: 54 million lines processed\n",
      "File page_links_en.ttl.bz2: 55 million lines processed\n",
      "File page_links_en.ttl.bz2: 56 million lines processed\n",
      "File page_links_en.ttl.bz2: 57 million lines processed\n",
      "File page_links_en.ttl.bz2: 58 million lines processed\n",
      "File page_links_en.ttl.bz2: 59 million lines processed\n",
      "File page_links_en.ttl.bz2: 60 million lines processed\n",
      "File page_links_en.ttl.bz2: 61 million lines processed\n",
      "File page_links_en.ttl.bz2: 62 million lines processed\n",
      "File page_links_en.ttl.bz2: 63 million lines processed\n",
      "File page_links_en.ttl.bz2: 64 million lines processed\n",
      "File page_links_en.ttl.bz2: 65 million lines processed\n",
      "File page_links_en.ttl.bz2: 66 million lines processed\n",
      "File page_links_en.ttl.bz2: 67 million lines processed\n",
      "File page_links_en.ttl.bz2: 68 million lines processed\n",
      "File page_links_en.ttl.bz2: 69 million lines processed\n",
      "File page_links_en.ttl.bz2: 70 million lines processed\n",
      "File page_links_en.ttl.bz2: 71 million lines processed\n",
      "File page_links_en.ttl.bz2: 72 million lines processed\n",
      "File page_links_en.ttl.bz2: 73 million lines processed\n",
      "File page_links_en.ttl.bz2: 74 million lines processed\n",
      "File page_links_en.ttl.bz2: 75 million lines processed\n",
      "File page_links_en.ttl.bz2: 76 million lines processed\n",
      "File page_links_en.ttl.bz2: 77 million lines processed\n",
      "File page_links_en.ttl.bz2: 78 million lines processed\n",
      "File page_links_en.ttl.bz2: 79 million lines processed\n",
      "File page_links_en.ttl.bz2: 80 million lines processed\n",
      "File page_links_en.ttl.bz2: 81 million lines processed\n",
      "File page_links_en.ttl.bz2: 82 million lines processed\n",
      "File page_links_en.ttl.bz2: 83 million lines processed\n",
      "File page_links_en.ttl.bz2: 84 million lines processed\n",
      "File page_links_en.ttl.bz2: 85 million lines processed\n",
      "File page_links_en.ttl.bz2: 86 million lines processed\n",
      "File page_links_en.ttl.bz2: 87 million lines processed\n",
      "File page_links_en.ttl.bz2: 88 million lines processed\n",
      "File page_links_en.ttl.bz2: 89 million lines processed\n",
      "File page_links_en.ttl.bz2: 90 million lines processed\n",
      "File page_links_en.ttl.bz2: 91 million lines processed\n",
      "File page_links_en.ttl.bz2: 92 million lines processed\n",
      "File page_links_en.ttl.bz2: 93 million lines processed\n",
      "File page_links_en.ttl.bz2: 94 million lines processed\n",
      "File page_links_en.ttl.bz2: 95 million lines processed\n",
      "File page_links_en.ttl.bz2: 96 million lines processed\n",
      "File page_links_en.ttl.bz2: 97 million lines processed\n",
      "File page_links_en.ttl.bz2: 98 million lines processed\n",
      "File page_links_en.ttl.bz2: 99 million lines processed\n",
      "File page_links_en.ttl.bz2: 100 million lines processed\n",
      "File page_links_en.ttl.bz2: 101 million lines processed\n",
      "File page_links_en.ttl.bz2: 102 million lines processed\n",
      "File page_links_en.ttl.bz2: 103 million lines processed\n",
      "File page_links_en.ttl.bz2: 104 million lines processed\n",
      "File page_links_en.ttl.bz2: 105 million lines processed\n",
      "File page_links_en.ttl.bz2: 106 million lines processed\n",
      "File page_links_en.ttl.bz2: 107 million lines processed\n",
      "File page_links_en.ttl.bz2: 108 million lines processed\n",
      "File page_links_en.ttl.bz2: 109 million lines processed\n",
      "File page_links_en.ttl.bz2: 110 million lines processed\n",
      "File page_links_en.ttl.bz2: 111 million lines processed\n",
      "File page_links_en.ttl.bz2: 112 million lines processed\n",
      "File page_links_en.ttl.bz2: 113 million lines processed\n",
      "File page_links_en.ttl.bz2: 114 million lines processed\n",
      "File page_links_en.ttl.bz2: 115 million lines processed\n",
      "File page_links_en.ttl.bz2: 116 million lines processed\n",
      "File page_links_en.ttl.bz2: 117 million lines processed\n",
      "File page_links_en.ttl.bz2: 118 million lines processed\n",
      "File page_links_en.ttl.bz2: 119 million lines processed\n",
      "File page_links_en.ttl.bz2: 120 million lines processed\n",
      "File page_links_en.ttl.bz2: 121 million lines processed\n",
      "File page_links_en.ttl.bz2: 122 million lines processed\n",
      "File page_links_en.ttl.bz2: 123 million lines processed\n",
      "File page_links_en.ttl.bz2: 124 million lines processed\n",
      "File page_links_en.ttl.bz2: 125 million lines processed\n",
      "File page_links_en.ttl.bz2: 126 million lines processed\n",
      "File page_links_en.ttl.bz2: 127 million lines processed\n",
      "File page_links_en.ttl.bz2: 128 million lines processed\n",
      "File page_links_en.ttl.bz2: 129 million lines processed\n",
      "File page_links_en.ttl.bz2: 130 million lines processed\n",
      "File page_links_en.ttl.bz2: 131 million lines processed\n",
      "File page_links_en.ttl.bz2: 132 million lines processed\n",
      "File page_links_en.ttl.bz2: 133 million lines processed\n",
      "File page_links_en.ttl.bz2: 134 million lines processed\n",
      "File page_links_en.ttl.bz2: 135 million lines processed\n",
      "File page_links_en.ttl.bz2: 136 million lines processed\n",
      "File page_links_en.ttl.bz2: 137 million lines processed\n",
      "File page_links_en.ttl.bz2: 138 million lines processed\n",
      "File page_links_en.ttl.bz2: 139 million lines processed\n",
      "File page_links_en.ttl.bz2: 140 million lines processed\n",
      "File page_links_en.ttl.bz2: 141 million lines processed\n",
      "File page_links_en.ttl.bz2: 142 million lines processed\n",
      "File page_links_en.ttl.bz2: 143 million lines processed\n",
      "File page_links_en.ttl.bz2: 144 million lines processed\n",
      "File page_links_en.ttl.bz2: 145 million lines processed\n",
      "File page_links_en.ttl.bz2: 146 million lines processed\n",
      "File page_links_en.ttl.bz2: 147 million lines processed\n",
      "File page_links_en.ttl.bz2: 148 million lines processed\n",
      "File page_links_en.ttl.bz2: 149 million lines processed\n",
      "File page_links_en.ttl.bz2: 150 million lines processed\n",
      "File page_links_en.ttl.bz2: 151 million lines processed\n",
      "File page_links_en.ttl.bz2: 152 million lines processed\n",
      "File page_links_en.ttl.bz2: 153 million lines processed\n",
      "File page_links_en.ttl.bz2: 154 million lines processed\n",
      "File page_links_en.ttl.bz2: 155 million lines processed\n",
      "File page_links_en.ttl.bz2: 156 million lines processed\n",
      "File page_links_en.ttl.bz2: 157 million lines processed\n",
      "File page_links_en.ttl.bz2: 158 million lines processed\n",
      "File page_links_en.ttl.bz2: 159 million lines processed\n",
      "File page_links_en.ttl.bz2: 160 million lines processed\n",
      "File page_links_en.ttl.bz2: 161 million lines processed\n",
      "File page_links_en.ttl.bz2: 162 million lines processed\n",
      "File page_links_en.ttl.bz2: 163 million lines processed\n",
      "File page_links_en.ttl.bz2: 164 million lines processed\n",
      "File page_links_en.ttl.bz2: 165 million lines processed\n",
      "File page_links_en.ttl.bz2: 166 million lines processed\n",
      "File page_links_en.ttl.bz2: 167 million lines processed\n",
      "File page_links_en.ttl.bz2: 168 million lines processed\n",
      "File page_links_en.ttl.bz2: 169 million lines processed\n",
      "File page_links_en.ttl.bz2: 170 million lines processed\n",
      "Indexing completed\n"
     ]
    }
   ],
   "source": [
    "actions = []\n",
    "# hold a set of indexed entities. For the entities already in the index, doing the update action using _op_type=update\n",
    "indexed_entities = set()\n",
    "for file in files:\n",
    "    filepath = os.path.join(DATA_PATH, DBPEDIA_PATH, file['name'])\n",
    "    reverse = file.get('reverse', False)\n",
    "    spo = SPO()\n",
    "    parser = NTriplesParser(spo)\n",
    "    count, j = 0, 0\n",
    "    with bz2.open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            count += 1\n",
    "            if count == 1000000:\n",
    "                j += 1\n",
    "                count = 0\n",
    "                print(\"File {}: {} million lines processed\".format(file['name'], j))\n",
    "            try:\n",
    "                # parse the triple\n",
    "                parser.parsestring(line.decode(\"utf-8\"))\n",
    "            except ParseError:\n",
    "                continue\n",
    "            \n",
    "            if spo.s is None or spo.o is None:\n",
    "                continue\n",
    "\n",
    "            subject = url_to_prefixed(spo.s)\n",
    "                \n",
    "            predicate = url_to_prefixed(spo.p)\n",
    "            if reverse:\n",
    "                predicate = \"!\" + predicate\n",
    "            \n",
    "            object = spo.o\n",
    "            # if object is a URI\n",
    "            if type(object) is URIRef:\n",
    "                object = url_to_prefixed(object)\n",
    "            \n",
    "            # reverse direction\n",
    "            if reverse:\n",
    "                temp = subject\n",
    "                subject = object\n",
    "                object = temp\n",
    "                \n",
    "            # only keep subject that is dbpedia entity\n",
    "            if not subject.startswith('<dbpedia:'):\n",
    "                continue\n",
    "            \n",
    "            # only keep entities that has both <rdfs:label> and <rdfs:comment>\n",
    "            if subject not in ENTITIES:\n",
    "                continue\n",
    "            \n",
    "            # Add top-1000 predicates fields into index\n",
    "            if predicate in TOP_PREDICATES:\n",
    "                if object.startswith('<') and object.endswith('>'):\n",
    "                    if subject in indexed_entities:\n",
    "                        actions.append({\n",
    "                            '_index': INDEX_URI_NAME,\n",
    "                            '_id': subject,\n",
    "                            'doc': {predicate: [object]},\n",
    "                            '_op_type': 'update',\n",
    "                        })\n",
    "                    else:\n",
    "                        actions.append({\n",
    "                            '_index': INDEX_URI_NAME,\n",
    "                            '_id': subject,\n",
    "                            '_source': {predicate: [object]}\n",
    "                        })\n",
    "                        indexed_entities.add(subject)\n",
    "\n",
    "            # Add entities into the index in a bulk of 5000 to improve the performance\n",
    "            if len(actions) > 5000:\n",
    "                helpers.bulk(es, actions)\n",
    "                actions = []\n",
    "                \n",
    "# process the last bulk\n",
    "helpers.bulk(es, actions)\n",
    "print('Indexing completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'dbpedia_index',\n",
       " '_type': '_doc',\n",
       " '_id': '<dbpedia:Rome>',\n",
       " '_version': 844,\n",
       " 'found': True,\n",
       " 'took': 0,\n",
       " 'term_vectors': {'catch-all': {'field_statistics': {'sum_doc_freq': 77805365,\n",
       "    'doc_count': 6564468,\n",
       "    'sum_ttf': 189950900},\n",
       "   'terms': {'heritage': {'term_freq': 2,\n",
       "     'tokens': [{'position': 1}, {'position': 106}]},\n",
       "    'italy': {'term_freq': 2, 'tokens': [{'position': 4}, {'position': 109}]},\n",
       "    'site': {'term_freq': 2, 'tokens': [{'position': 2}, {'position': 107}]},\n",
       "    'world': {'term_freq': 2,\n",
       "     'tokens': [{'position': 0}, {'position': 105}]}}}}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv = es.termvectors(index=INDEX_NAME, id='<dbpedia:Rome>', fields=\"catch-all\")\n",
    "tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dbo:wikiPageWikiLink>\n",
      "{'_index': 'dbpedia_index', '_type': '_doc', '_id': '<dbpedia:Outline_of_cuisines>', '_version': 83, 'found': True, 'took': 0, 'term_vectors': {}}\n",
      "<dbo:abstract>\n",
      "{'_index': 'dbpedia_index', '_type': '_doc', '_id': '<dbpedia:Outline_of_cuisines>', '_version': 83, 'found': True, 'took': 0, 'term_vectors': {}}\n",
      "<rdfs:label>\n",
      "{'_index': 'dbpedia_index', '_type': '_doc', '_id': '<dbpedia:Outline_of_cuisines>', '_version': 83, 'found': True, 'took': 0, 'term_vectors': {}}\n"
     ]
    }
   ],
   "source": [
    "for p in TOP_PREDICATES:\n",
    "    print(p)\n",
    "    tv = es.termvectors(index=INDEX_NAME, id='<dbpedia:Outline_of_cuisines>', fields=p)\n",
    "    print(tv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
